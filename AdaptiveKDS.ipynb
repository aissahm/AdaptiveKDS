{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aissahm/AdaptiveKDS/blob/main/AdaptiveKDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **AdaptiveKDS**"
      ],
      "metadata": {
        "id": "54XhxXtCDuDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!  chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/gtsrb-german-traffic-sign.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")"
      ],
      "metadata": {
        "id": "vI34GYVkGFkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "from tensorflow import keras\n",
        "import random as random\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "2oQiCfmYGKLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir):\n",
        "  rawimages = []\n",
        "  rawlabels = []\n",
        "  for label in os.listdir(data_dir):\n",
        "      label_path = os.path.join(data_dir, label)\n",
        "      for image_file in os.listdir(label_path):\n",
        "          image_path = os.path.join(label_path, image_file)\n",
        "          image = cv2.imread(image_path)\n",
        "          image = cv2.resize(image, (32, 32))  # Resize images to 32x32 pixels\n",
        "          rawimages.append(image)\n",
        "          rawlabels.append(int(label))  # Convert folder name (label) to integer\n",
        "  rawimages = np.array(rawimages)\n",
        "  rawlabels = np.array(rawlabels)\n",
        "  return rawimages, rawlabels"
      ],
      "metadata": {
        "id": "2BAUXgloGMbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images,labels = load_data('/content/Train')\n",
        "\n",
        "# Normalize the images\n",
        "normalized_images = images / 255.0"
      ],
      "metadata": {
        "id": "9eeyq8dwGQWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "#given the dataset X, Y, the object with indexes for every client, returns the dataset of client identified with its client_id\n",
        "def returnClientDataset(client_id, clients_data_obj, x, y):\n",
        "  dataset_indexes = np.array(clients_data_obj[client_id][\"indexes\"])\n",
        "  return [x[dataset_indexes], y[dataset_indexes]]\n",
        "\n",
        "##FL parameters\n",
        "clients_datasets_obj_filename = \"/content/German_Traffic_Sign_100_clients_alpha_75_third.pickle\"\n",
        "clients_datasets_obj = pickle.load( open(clients_datasets_obj_filename, \"rb\" ) )"
      ],
      "metadata": {
        "id": "31wigYijGV3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "  plt.figure(figsize=(5, 3))\n",
        "  plt.plot(history['accuracy'], label= history['exp_name'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Value')\n",
        "  plt.title('Training History')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "def knowledge_distillation_loss(y_true, y_pred):\n",
        "  y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
        "  # Ensure that y_pred has the same shape as soft targets\n",
        "  y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
        "\n",
        "  loss_ce = losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
        "  return loss_ce\n",
        "\n",
        "# Define a simple CNN model\n",
        "def create_model():\n",
        "  model = Sequential([\n",
        "    Input(shape=(32, 32, 3)),\n",
        "    Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(43, activation='softmax')\n",
        "  ])\n",
        "  model.compile(loss=knowledge_distillation_loss, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "#Given the weights after training and initial weights, returns the gradient from entire training\n",
        "def computeClientGradientNoCompression(modelNotTrained, modelTrained):\n",
        "  gradient = []\n",
        "  notTrainedWeight = modelNotTrained.get_weights()\n",
        "  i = 0\n",
        "  for weight in modelTrained.get_weights():\n",
        "    gradient.append( notTrainedWeight[i] - weight )\n",
        "    i += 1\n",
        "  return gradient\n",
        "\n",
        "#add the client gradient to the global model\n",
        "def addGradientNoCompression(modelNotTrained, gradient, clientweight):\n",
        "  newWeight = []\n",
        "  i = 0\n",
        "  for weight in modelNotTrained.get_weights():\n",
        "    newWeight.append( weight - (gradient[i] * clientweight) )\n",
        "    i += 1\n",
        "  modelNotTrained.set_weights(newWeight)\n",
        "  return newWeight\n",
        "\n",
        "# Federated averaging function\n",
        "def federated_avg(teacher_model, models, clientweightslst):\n",
        "  gradients = []\n",
        "  for model in models:\n",
        "    gradient = computeClientGradientNoCompression(teacher_model, model)\n",
        "    gradients.append(gradient)\n",
        "  for i in range(len(models)):\n",
        "    addGradientNoCompression(teacher_model, gradients[i], , clientweightslst[i])\n",
        "  return teacher_model.get_weights()\n",
        "\n",
        "# Federated training function for each client\n",
        "def train_client(model, data, teacher_model, lambdaval):\n",
        "  (client_x_train, client_y_train) = data\n",
        "\n",
        "  soft_target_train = teacher_model.predict(client_x_train)\n",
        "\n",
        "  def student_knowledge_distillation_loss(y_true, y_pred):\n",
        "    loss_ce = losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
        "    loss_kd = tf.keras.losses.KLD(soft_target_train, tf.nn.softmax(y_pred / temperature))\n",
        "    return (1- lambdaval)*loss_ce + lambdaval * loss_kd  # Adjust the weight for the distillation loss as needed\n",
        "\n",
        "  # Compile model with combined loss\n",
        "  model.compile(loss=student_knowledge_distillation_loss,\n",
        "                optimizer='adam', metrics=['accuracy'])\n",
        "  model.set_weights(teacher_model.get_weights())\n",
        "  model.fit(client_x_train, client_y_train, epochs=2, validation_split = .0 ,verbose=0,  batch_size=1)\n",
        "  return model\n",
        "\n",
        "#returns a copy of the global model to client\n",
        "def returnCopyGlobalModelToClient(globalmodel):\n",
        "  clientmodel = create_model()\n",
        "  clientmodel.set_weights(globalmodel.get_weights())\n",
        "  return clientmodel"
      ],
      "metadata": {
        "id": "0O7HSnSLGsta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **setting the experiment parameters**"
      ],
      "metadata": {
        "id": "5KNTTMXfISeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiments_obj = []\n",
        "\n",
        "num_rounds = 35\n",
        "participating_clients_per_round = 8\n",
        "clients_per_round = 4\n",
        "\n",
        "num_clients = 90\n",
        "\n",
        "x_train, y_train = [normalized_images, labels]\n",
        "\n",
        "test_indexes = []\n",
        "for i in range(num_clients, len(clients_datasets_obj)):\n",
        "  test_indexes += list(clients_datasets_obj[i][\"indexes\"])\n",
        "x_test, y_test = [ x_train[test_indexes], y_train[test_indexes]  ]\n",
        "\n",
        "train_indexes = []\n",
        "for i in range(0, num_clients):\n",
        "  train_indexes += list(clients_datasets_obj[i][\"indexes\"])\n",
        "\n",
        "x_train_aggregated, y_train_aggregated = [x_train[train_indexes], y_train[train_indexes]]"
      ],
      "metadata": {
        "id": "sebiYlZBGfBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##KD parameters\n",
        "experiment_name = \"lambda=min(accuracy, 0.5)\"\n",
        "temperature = 5"
      ],
      "metadata": {
        "id": "UiRygjGJIVJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######training starts\n",
        "initial_model = create_model()\n",
        "teacher_model = returnCopyGlobalModelToClient(initial_model)\n",
        "\n",
        "\n",
        "# Initialize lists to store test accuracies and losses\n",
        "test_accuracies = []\n",
        "test_losses = []\n",
        "\n",
        "# Evaluate teacher model on test data initially\n",
        "loss, acc = teacher_model.evaluate(x_test, y_test)\n",
        "test_accuracies.append(acc)\n",
        "test_losses.append(loss)\n",
        "print(f\"Round:{0}, Test Loss: {loss}, Test Accuracy: {acc}\")\n",
        "\n",
        "#evaluate teacher model on local data initially\n",
        "loss, acc = teacher_model.evaluate(x_train_aggregated, y_train_aggregated)\n",
        "average_local_accuracy_list.append(acc)\n",
        "print(f\"Round:{0}, Local Loss: {loss}, Local Accuracy: {acc}\")\n",
        "\n",
        "\n",
        "for round in range(0, num_rounds):\n",
        "  # Select participating clients for this round\n",
        "  random_selected_clients = random.sample(range(num_clients), participating_clients_per_round)\n",
        "\n",
        "  #select clients for training\n",
        "  clients_accuracy_list = []\n",
        "  for i, client_id in enumerate(random_selected_clients):\n",
        "    client_data_i = returnClientDataset(client_id, clients_datasets_obj, x_train, y_train)\n",
        "    loss, acc = teacher_model.evaluate(client_data_i[0], client_data_i[1])\n",
        "    clients_accuracy_list.append({\"clientID\": client_id, \"accuracy\": acc})\n",
        "\n",
        "  # POC strategy\n",
        "  #order the accuracies from worst to best\n",
        "  clients_accuracy_list.sort(key=lambda x: x[\"accuracy\"], reverse=False)\n",
        "\n",
        "  print(\"clients_accuracy_list: \", clients_accuracy_list)\n",
        "\n",
        "  selected_clients = [item['clientID'] for item in clients_accuracy_list[:clients_per_round]]\n",
        "\n",
        "  # Create client models (one per selected client)\n",
        "  client_models = [returnCopyGlobalModelToClient(teacher_model) for _ in selected_clients]\n",
        "\n",
        "  lambda_values = []\n",
        "  clients_weights_list = []\n",
        "  for i, client_id in enumerate(selected_clients):\n",
        "    lambda_value = min(clients_accuracy_list[i]['accuracy'], 0.5)\n",
        "    lambda_values.append(lambda_value)\n",
        "    client_data_i = returnClientDataset(client_id, clients_datasets_obj, x_train, y_train)\n",
        "    client_models[i] = train_client(client_models[i], client_data_i, teacher_model, lambda_value)\n",
        "    clients_weights_list.append( client_data_i[1].shape[0] / y_train.shape[0] )\n",
        "\n",
        "  print(lambda_values)\n",
        "\n",
        "  # Update teacher model with FedAvg\n",
        "  teacher_model.set_weights(federated_avg(teacher_model, client_models, clients_weights_list))\n",
        "\n",
        "  # Evaluate teacher model on test data\n",
        "  loss, acc = teacher_model.evaluate(x_test, y_test)\n",
        "  test_accuracies.append(acc)\n",
        "  test_losses.append(loss)\n",
        "  print(f\"Round:{round+1}, Test Loss: {loss}, Test Accuracy: {acc}\")\n",
        "\n",
        "  print()\n",
        "\n",
        "# evaluate teacher model on local data\n",
        "clients_accuracy_list = []\n",
        "for i, client_id in enumerate(range(0, num_clients)):\n",
        "  client_data_i = returnClientDataset(client_id, clients_datasets_obj, x_train, y_train)\n",
        "  loss, acc = teacher_model.evaluate(client_data_i[0], client_data_i[1])\n",
        "  clients_accuracy_list.append({\"clientID\": client_id, \"accuracy\": acc})\n",
        "\n",
        "# saving the training results\n",
        "experiments_obj.append({\"exp_name\": experiment_name, \"accuracy\": test_accuracies, \"loss\": test_losses, \"clients_accuracy_list\": clients_accuracy_list})\n",
        "plot_history(experiments_obj[-1])"
      ],
      "metadata": {
        "id": "MZdvnMl-HZ5S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}